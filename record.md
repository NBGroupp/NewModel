[markdown语法参考](http://blog.csdn.net/ljc_563812704/article/details/53464039 "打开链接") 

# 项目记录

## 2017年4月-2017年12月，记录人：刘明桓

### 任务摘要

此阶段将模型架构定为sequence-to-sequence，使用整句作为输入，整句作为输出。主要花费时间学习和尝试encoder-decoder模型，包括大量阅读论文等。尝试了character-level/word-level的encoder-decoder RNN，用于文本校对，即文本字词的分类问题。

类比分类器，制定了校对系统的评价方法。

对于character-level/word-level的encoder-decoder RNN：

使用自定义的随机混错算法进行随机混错训练，使用了5%，10%，20%，40%的混错，但效果不佳，推测因混错较为随机，学习的成本太大。在训练集中训练得到的cost较低，但集中于原本正确的部分，错误的部分绝大多数没有校对出。在测试集中，校对系统对正确和错误的位置均发生了预测的错误，效果很差。

使用全正确文本进行学习和输出，原意是希望学习到句子相关信息，在训练集上效果良好，cost甚至降为0，但在测试集加入混错以后，校对系统对正确和错误的位置均发生了预测的错误，效果很差。

### 结果简述

使用character-level/word-level的encoder-decoder RNN，均：

使用混错样本训练，在测试集上，校对系统对正确和错误的位置均发生了预测的错误，正确字词校对的概率很高，错误字词校对的概率很低，总体效果很差。

使用正确样本训练，在测试集上，校对系统对正确和错误的位置均发生了预测的错误，正确字词校对的概率很高，错误字词校对的概率很低，总体效果很差。

## 2018年1月10日-2018年1月13日，记录人：刘明桓

### 任务摘要

这段时间，放弃了之前的encoder-decoder模型，转向了词向量-双向lstm，将原本针对sequence-to-sequence，一句话作为输入输出的方向，变更为针对词语，使用完全正确的文本作为输入进行训练，使用词语附近的词语情况预测词语的分类，对于错误的词语，不会带入错误的信息。

采用第三方维基百科-百度百科收集的词语制定词典vocabulary，统计频率最高的10w个词语作为最终的词典，不在词典中的单词将编码为unk。

模型大意：定义上文向量和下文向量，制定了两个规则：

fixed-length：上文向量即词语前面的n个词语（不够n个使用填充符填充），下文向量同理。

unfixed-length：上文向量即词语前面的所有词语（无词语使用1个填充符填充），下文向量同理。

one-hot编码的输入进入后使用embedding层将词语映射到维度为100的向量空间中（hidden-size）

上下文向量分别使用lstm得到预测结果，预测结果计算算数平均值（或拼接）后，连接全连接层，再使用soft-max层将输出映射为vocabulary-size的概率分布，使用交叉熵损失函数计算损失。

### 结果简述

采用1000句-18000条左右的训练集，在训练集上的效果可以达到85%以上。但是在测试集中效果只有3%左右。正在进行调参和模型的改进。

## 2018年1月14日，记录人：王秋锋

### 任务摘要
在原有的词库基础上增加了现代汉语词汇和通用字表，改进了unk语句的查找算法，速度由几小时变为不到一分钟。至此数据已经全部处理完成。

开始用pytorch搭建lstm神经网络。

### 结果简述

刚开始读入大数据跑模型，在测试集中效果有12%左右。



## 2018年1月15日，记录人：郭宇航

### 任务摘要

引进哈工大的词性标注库，抽出语料中的人名与地名进行替换为 ‘N’ 和 ‘P’，重新建立字典。

测试新字典和旧字典在平衡语料库中的Token覆盖率，语料库的UNK句子数目。

### 结果简述

用旧字典对平衡语料库建立输入输出对时：

unk token in total tokens: 808224/11341918  7.13%
unk sentence in total sentences: 352620/527629  66.83%
2 more unks sentence number in total_sentence_number: 201019/527629  38.10%

用新字典对平衡语料库建立输入输出对时：

Normalizing data...  done.%
unk token in total tokens: 455148/12064876  3.77%
unk sentence in total sentences: 246522/527629  46.72%
2 more unks sentence number in total_sentence_number: 108557/527629  20.57%

从结果来看，新字典的UNK比率比旧字典下降了很多。