[markdown语法参考](http://blog.csdn.net/ljc_563812704/article/details/53464039 "打开链接") 

# 项目记录

## 2017年4月-2017年12月，记录人：刘明桓

### 任务摘要

此阶段将模型架构定为sequence-to-sequence，使用整句作为输入，整句作为输出。主要花费时间学习和尝试encoder-decoder模型，包括大量阅读论文等。尝试了character-level/word-level的encoder-decoder RNN，用于文本校对，即文本字词的分类问题。

类比分类器，制定了校对系统的评价方法。

对于character-level/word-level的encoder-decoder RNN：

使用自定义的随机混错算法进行随机混错训练，使用了5%，10%，20%，40%的混错，但效果不佳，推测因混错较为随机，学习的成本太大。在训练集中训练得到的cost较低，但集中于原本正确的部分，错误的部分绝大多数没有校对出。在测试集中，校对系统对正确和错误的位置均发生了预测的错误，效果很差。

使用全正确文本进行学习和输出，原意是希望学习到句子相关信息，在训练集上效果良好，cost甚至降为0，但在测试集加入混错以后，校对系统对正确和错误的位置均发生了预测的错误，效果很差。

### 结果简述

使用character-level/word-level的encoder-decoder RNN，均：

使用混错样本训练，在测试集上，校对系统对正确和错误的位置均发生了预测的错误，正确字词校对的概率很高，错误字词校对的概率很低，总体效果很差。

使用正确样本训练，在测试集上，校对系统对正确和错误的位置均发生了预测的错误，正确字词校对的概率很高，错误字词校对的概率很低，总体效果很差。

## 2018年1月10日-2018年1月13日，记录人：刘明桓

### 任务摘要

这段时间，放弃了之前的encoder-decoder模型，转向了词向量-双向lstm，将原本针对sequence-to-sequence，一句话作为输入输出的方向，变更为针对词语，使用完全正确的文本作为输入进行训练，使用词语附近的词语情况预测词语的分类，对于错误的词语，不会带入错误的信息。

采用第三方维基百科-百度百科收集的词语制定词典vocabulary，统计频率最高的10w个词语作为最终的词典，不在词典中的单词将编码为unk。

模型大意：定义上文向量和下文向量，制定了两个规则：

fixed-length：上文向量即词语前面的n个词语（不够n个使用填充符填充），下文向量同理。

unfixed-length：上文向量即词语前面的所有词语（无词语使用1个填充符填充），下文向量同理。

one-hot编码的输入进入后使用embedding层将词语映射到维度为100的向量空间中（hidden-size）

上下文向量分别使用lstm得到预测结果，预测结果计算算数平均值（或拼接）后，连接全连接层，再使用soft-max层将输出映射为vocabulary-size的概率分布，使用交叉熵损失函数计算损失。

### 结果简述

采用1000句-18000条左右的训练集，在训练集上的效果可以达到85%以上。但是在测试集中效果只有3%左右。正在进行调参和模型的改进。

## 2018年1月14日，记录人：王秋锋

### 任务摘要
在原有的词库基础上增加了现代汉语词汇和通用字表，改进了unk语句的查找算法，速度由几小时变为不到一分钟。至此数据已经全部处理完成。

开始用pytorch搭建lstm神经网络。

### 结果简述

刚开始读入大数据跑模型，在测试集中效果有12%左右。



## 2018年1月15日，记录人：郭宇航

### 任务摘要

引进哈工大的词性标注库，抽出语料中的人名与地名进行替换为 ‘N’ 和 ‘P’，重新建立字典。

测试新字典和旧字典在平衡语料库中的Token覆盖率，语料库的UNK句子数目。

### 结果简述

用旧字典对平衡语料库建立输入输出对时：

unk token in total tokens: 808224/11341918  7.13%

unk sentence in total sentences: 352620/527629  66.83%

2 more unks sentence number in total_sentence_number: 201019/527629  38.10%

用新字典对平衡语料库建立输入输出对时：

unk token in total tokens: 455148/12064876  3.77%

unk sentence in total sentences: 246522/527629  46.72%

2 more unks sentence number in total_sentence_number: 108557/527629  20.57%

从结果来看，新字典的UNK比率比旧字典下降了很多。

# 数据处理

## 原始数据格式

原始数据有三份：

- 国家平衡语料库
- 百度百科
- 中文维基

每份都为一个单独的文本文件，压缩为一个gz包。
文本文件里面为一句一行，未清洗不明字符，未替换数字、字母等。

## 数据预处理

1. 清洗原始语料库

    清洗操作包括：

    - 全角转半角
    - 繁体转简体


    - 删除空行
    - 删除汉字过少的句子（阈值现定为3）
    - 删除除汉字外字符的数量大于汉字数量的句子
    - 删除第一个字符不为汉字的句子
    - 删除引号未成对的句子
    - 可选：删除包含了除标点、汉字、字母、数字的字符的句子
    
    ​
    对应函数为 `data_util.py` 中的 `clean_corpus(data_path, replace=False, strict=False)`：
    
    ​```
    Args：
    data_path: 原始语料库gz包路径
    replace: 可选，若为 True，保存清洗后的新语料库gz包
    strict：可选，若为 True，删除包含了除标点、汉字、字母、数字的字符的句子
    
    Return：
    new_corpus_data：清洗后语料库，列表，元素为一个句子
    ​```

2. 替换字符

    可替换的字符包括：标点、字母、数字（包括浮点数和百分数）、除汉字字母数字以外的字符

    对应函数为 `data_util.py` 中的 `normalize_corpus_data`：

    ```
    Args：
    corpus_data：句子列表
    normalize_char：可选，若为 True，替换句子中的字母（单词）为 ‘a’
    normalize_digits：可选，若为 True，替换句子中的数字（浮点数、百分数）为 ‘0’
    normalize_punctuation：可选，若为 True，替换句子中的标点为 ‘.’，替换的标点在 `data_util.py` 中的 `puncs` 字符串中定义
    normalize_others：可选，若为 True，替换句子中的除汉字字母数字以外的字符为 'o'

    Return：
    p_corpus_data：替换后语料库，列表，元素为一个句子
    ```



## 建立字典

对应函数为 `data_util.py` 中的 `create_vocabulary`：

```
Args：
corpus_data：句子列表
max_vocabulary_size：字典最大大小，若为 -1, 则不限制大小
tokenizer：序列化函数，默认为 `data_util.py` 中的 `cut_word_tokenizer`，jeiba全模式分词

Return：
to_save_vocab_list：大小为 max_vocabulary_size 的字典列表
vocab：原始字典列表
tokenized_data：序列化后的语料库列表，二维列表
```

## 2018年1月16日，记录人：张韦嘉

### 任务摘要

1.今日用不定长的LSTM跑了一遍数据，发现效果显著。
  不定长相比定长的LSTM模型的区别在于，
  定长的模型是利用待预测词的前后三个词进行输入，
  而不定长的模型则是利用一个句子待预测词的前后所有词进行输入，当前后空缺时用一个字符进行填充。
  定长LSTM（前后取3词）模型测试集准确不过30%。
  而不定长的LSTM模型，训练初，验证集的正确率就达到了惊人的86%，令人振奋。
2.对数据进行了进一步的清洗。

### 结果简述
不定长LSTM效果显著，训练了2轮，正确率就高达86%，它的进一步表现令人期待。
## 2018年1月17日，记录人：刘明桓

### 任务摘要

1.今日继续使用旧数据跑昨天尝试的不定长LSTM模型，由于代码问题，在凌晨1点半才重新开始跑，晚上查看模型效果。
2.和学长讨论论文写法和实验进展，开始准备论文的写作，阅读大量论文，揣摩论文框架。

### 结果简述

1.在当前的参数下，不定长LSTM的模型效果在5-6轮后就已经收敛，loss无法继续降低
2.实验的模型仍旧无法保存，保存后再读取，模型就会出错，效果很差。经排查不是因为数据读错导致的过拟合问题，因为保存的模型在训练集上效果也不如先前训练过程中的效果。认为是保存和读取的问题，正在解决。   

## 2018年1月18日，记录人：刘明桓

### 任务摘要

1.解决模型无法保存的问题。今天在小数据集上认为解决了模型无法保存的问题。
2.使用清洗好的最新数据，进行完整的实验，训练轮数为20轮。

### 结果简述

1.原因初步认为是在训练过程中，运行收集节点时，没有feed数据导致的数据错误，尚不清楚为什么
2.效果和之前差不多

## 2018年1月20日，记录人：刘明桓

### 任务摘要

1.张韦嘉解决模型无法保存的问题。
2.继续写论文，发现了原有的下文数据的问题，郭宇航对数据进行了修正，需要重新训练
3.经过讨论，训练语料中需要有unk，暂定20%，10——40%的效果不明，需要用训练来说明超过多少比例说明句子信息受到极大影响，影响训练。

### 结果简述

1.模型需要读取两次，尚不清楚原因
2.机房电脑宕机，需要重装系统

